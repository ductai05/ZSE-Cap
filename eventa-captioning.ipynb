{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11855771,"sourceType":"datasetVersion","datasetId":7449612},{"sourceId":11855897,"sourceType":"datasetVersion","datasetId":7449685},{"sourceId":11856994,"sourceType":"datasetVersion","datasetId":7449575},{"sourceId":12135428,"sourceType":"datasetVersion","datasetId":7465410},{"sourceId":12137947,"sourceType":"datasetVersion","datasetId":7495521},{"sourceId":12156588,"sourceType":"datasetVersion","datasetId":7639209},{"sourceId":282751,"sourceType":"modelInstanceVersion","modelInstanceId":239470,"modelId":222398}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nGemma 3 model was launched March 12, 2025 in 4 sizes, each with both pretrained (pt) and instruction (fine)tuned (it), as following:\n\n* **1B** - input: 8K, output: 8K, text input and output \n* **4B** - input: 128K, output: 8K, multimodal input, text output \n* **12B** - input: 128K, output: 8K, multimodal input, text output \n* **27B**- input: 128K, output: 8K, multimodal input, text output \n\nIn this Notebook we will use the `gemma-3/transformers/gemma-3-4b-it/1` and `gemma-3/transformers/gemma-3-27b-it/1`\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Instalations\n\nWe install the latest version of transformers, compatible with Gemma3.","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3 -q --no-cache\n!pip install accelerate bitsandbytes","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import packages\n\nWe import pipeline from transformers library and torch library (we will use to set the parameters type). We also print each package version so that our results remain reproductible.","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\nfrom transformers import pipeline, AutoProcessor\nimport os\nimport sys\nimport pkg_resources\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,garbage_collection_threshold:0.6\"\n\nprint(\"Python:\", sys.version.replace(\"\\n\", \" \"))","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Intialize the pipeline","metadata":{}},{"cell_type":"code","source":"# point at the local repo and let HF pull in the custom classes\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"/kaggle/input/gemma-3/transformers/gemma-3-4b-it/1\",\n    revision=\"main\",                 # or the actual branch/tag if diff\n    trust_remote_code=True,          # <— allow custom Gemma3* classes to be loaded\n    device_map=\"auto\",\n    load_in_8bit=True,               # quantize weights to 8‑bit\n    torch_dtype=torch.bfloat16,      # mixed bfloat16 math\n    low_cpu_mem_usage=True,          # minimize CPU RAM spikes\n    offload_folder=\"/kaggle/working/offload\"\n)\n\n# the processor is auto‐discovered, but here we choose to explicitly do:\nprocessor = AutoProcessor.from_pretrained(\n    \"/kaggle/input/gemma-3/transformers/gemma-3-4b-it/1\",\n    trust_remote_code=True\n)\n\n# re-wrap so the processor is used\npipe = pipeline(\n    \"image-text-to-text\",\n    model=pipe.model,\n    processor=processor,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    use_cache=False                  # disable KV cache to save ~0.3 GiB\n)\n\nprint(\"Successfully loaded Gemma 3!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T02:39:09.689669Z","iopub.execute_input":"2025-06-13T02:39:09.690278Z","iopub.status.idle":"2025-06-13T02:40:33.359855Z","shell.execute_reply.started":"2025-06-13T02:39:09.690255Z","shell.execute_reply":"2025-06-13T02:40:33.359075Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Try simple multimodal prompts\n\n\nWe create a message where we specify the type of inputs (image & text) and we provide the url for image and the text.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport json\n\nfolder_img_path = \"/kaggle/input/eventa-img-cieldt/database_images_compressed90/\"\n\ndf_submission = pd.read_csv(\"/kaggle/input/eventa-private-submission/submission_ensembleScore.csv\")\n\nwith open(\"/kaggle/input/eventa-json-cieldt/database.json\") as f:\n    database = json.load(f)\n    f.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T02:49:32.228710Z","iopub.execute_input":"2025-06-13T02:49:32.229024Z","iopub.status.idle":"2025-06-13T02:50:00.189603Z","shell.execute_reply.started":"2025-06-13T02:49:32.228996Z","shell.execute_reply":"2025-06-13T02:50:00.188888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nfor i in range(0, 2000):\n    # --- 1. Lấy thông tin từ ensemble score ---\n    query_id = df_submission.loc[i, \"query_id\"]\n    retrieved_img_id = df_submission.loc[i, \"retrieved_id_img\"]\n    article_id = df_submission.loc[i, \"article_id_1\"]\n    article_content = 'date: ' + database[article_id]['date'] + 'title: ' + database[article_id]['title'] + 'content: ' + database[article_id]['content']\n    if (len(article_content) > 15000):\n        article_content = article_content[:15000]\n    if (i % 10 == 0):\n        print(i, \", len article_content:\", len(article_content))\n    \n    # --- 2. Đường dẫn đến ảnh ---\n    folder_img_path = \"/kaggle/input/eventa-img-cieldt/database_images_compressed90/\" \n    image_path = folder_img_path + retrieved_img_id + \".jpg\"\n    # !? error img\n    if (retrieved_img_id == '3a565cf1ad272aaf'):\n        image_path = '/kaggle/input/eventa-img-exception-cieldt/3a565cf1ad272aaf.jpg'\n    \n    # --- 3. Xây dựng prompt ---\n    prompt = \"\"\"You are a seasoned expert in journalistic photo caption writing. Your primary goal is to create a SINGLE, comprehensive caption that **masterfully connects the provided IMAGE to the key events, figures, and narratives within the ARTICLE.**\n    \n    This caption must achieve the following, with a strong emphasis on the connection to the article:\n    \n    1.  **Contextualize the Image through the Article First:**\n        *   Before describing visual details, understand the **core message, main event, or central figures discussed in the ARTICLE.**\n        *   Identify **how the IMAGE serves to illustrate, support, or provide a visual anchor for these key aspects of the article.**\n    \n    2.  **Describe the Image in Service of the Article's Narrative:**\n        *   Briefly describe the **most relevant visual elements of the IMAGE** – focusing on subjects, actions, and settings that directly pertain to the article's content.\n        *   **Prioritize details that help the reader understand the image's role in the story.** Less critical visual minutiae can be omitted if they don't serve this primary purpose.\n        *   If specific named entities (people, places, organizations) from the article are clearly visible and relevant, ensure they are mentioned to strengthen the link.\n    \n    3.  **Clearly Articulate the Significance and Connection:**\n        *   The caption's main thrust should be to clarify the **image's significance and its direct relationship to the information presented in the article.**\n        *   Ensure the reader understands *why* this specific image accompanies this article.\n    \n    4.  **Professional and Informative Style:**\n        *   Use precise, informative, and coherent language.\n        *   The style should be journalistic, conveying information efficiently and accurately.\n    \n    **Crucial Requirement:** Begin writing the caption IMMEDIATELY. Do not include any introductory phrases such as 'Here is the caption:', 'The caption is:', or similar.\n    \"\"\"\n    requirement_hard = f'''**Crucial Requirement:** Begin writing the caption IMMEDIATELY. Do not include any introductory phrases such as 'Here is the caption:', 'The caption is:', or similar.....'''\n    \n    # --- 4. Cấu trúc messages ---\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"url\": image_path}, # Ảnh đi kèm\n                {\"type\": \"text\", \"text\": prompt}, # Prompt\n                {\"type\": \"text\", \"text\": article_content}, # Article content\n                {\"type\": \"text\", \"text\": requirement_hard}, # Only generate caption      \n            ]\n        }\n    ]\n    \n    # --- 5. Chạy model và lưu + in kết quả ---\n    # %%time\n    output = pipe(text=messages, max_new_tokens=1500)\n    if (i % 10 == 0):\n        print(output[0][\"generated_text\"][-1][\"content\"])\n\n    df_submission.loc[i, \"generated_caption\"] = output[0][\"generated_text\"][-1][\"content\"]\n\n    # --- 6. Giải phóng bộ nhớ ---\n    del output\n    del messages\n    del article_content \n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T02:53:36.046969Z","iopub.execute_input":"2025-06-13T02:53:36.047250Z","iopub.status.idle":"2025-06-13T02:53:54.313694Z","shell.execute_reply.started":"2025-06-13T02:53:36.047229Z","shell.execute_reply":"2025-06-13T02:53:54.312979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_submission.to_csv(\"submission_private.csv\", index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T02:56:25.363223Z","iopub.execute_input":"2025-06-13T02:56:25.363590Z","iopub.status.idle":"2025-06-13T02:56:25.387241Z","shell.execute_reply.started":"2025-06-13T02:56:25.363560Z","shell.execute_reply":"2025-06-13T02:56:25.386594Z"}},"outputs":[],"execution_count":null}]}